{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 620, Project 3\n",
    "July 10, 2019 \n",
    "Team 6: Alice Friedman, Scott Jones, Jeff Littlejohn, and Jun Pan\n",
    "\n",
    "## Assignment Description\n",
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can. Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the devtest set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set. How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?\n",
    "\n",
    "Source: Natural Language Processing with Python, exercise 6.10.2.\n",
    "\n",
    "### Text Classification: Identifying Gender from the ```NLTK``` Names Corpus  \n",
    "\n",
    "Adapted from:\n",
    "\n",
    "- [GitHub, Vinovator](https://gist.github.com/vinovator/6e5bf1e1bc61687a1e809780c30d6bf6)\n",
    "\n",
    "- [Geeks for Geeks: Python Gender Identification by Name](https://www.geeksforgeeks.org/python-gender-identification-by-name-using-nltk/)\n",
    "\n",
    "\n",
    "First, we import the names corpus from the ```nltk``` list of corpuses, and create three sets of names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Patrice', 'male'), ('Sheffield', 'male'), ('Web', 'male'), ('Northrop', 'male'), ('Jeremias', 'male')] 2943\n",
      "[('Elfie', 'female'), ('Diamond', 'female'), ('Joana', 'female'), ('Karrie', 'female'), ('Nicole', 'female')] 5001\n",
      "[('Lucila', 'female'), ('Siana', 'female'), ('Emilio', 'male'), ('Horatius', 'male'), ('Vivie', 'female')] 7944\n"
     ]
    }
   ],
   "source": [
    "mcorpus = [(name, \"male\") for name in names.words(\"male.txt\")]\n",
    "fcorpus = [(name, \"female\") for name in names.words(\"female.txt\")]\n",
    "random.shuffle(mcorpus); random.shuffle(fcorpus)\n",
    "print(mcorpus[0:5],len(mcorpus))\n",
    "print(fcorpus[0:5],len(fcorpus))\n",
    "\n",
    "corpus = mcorpus + fcorpus\n",
    "random.shuffle(corpus)\n",
    "print(corpus[:5], len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2,943 names classified as male, 5,001 names classified as female, and 7,944 names total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will subdivide the shuffled names corpus as follows:\n",
    "\n",
    "- A training set, used to train the model based on our selected features\n",
    "\n",
    "- A development test (dev-test) set, which we will use to test progress on the gender identifier and perform error analysis\n",
    "\n",
    "- A final \"test\" set, which we will use to test how well our predictions ultimately worked\n",
    "\n",
    "In order to avoid overfitting the data, we will remix the training and dev-test with each new feature extraction model. To prevent a lot of re-coding, we can write a function to remix the development-training corpus, after setting aside the first 500 male and female names as the final test slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to return a new training and dev-test mix of the corpus for each iteration of the model\n",
    "def reslicer(corpus):\n",
    "    \n",
    "    #prints message to explain output\n",
    "    print(\"Reslicer returns 3 sliced, remixed set of corpuses:\")\n",
    "    print(\"\\tThe first returned value is the remixed training corpus, length is variable\")\n",
    "    print(\"\\tThe second returned value is the remixed dev-test corpus, length is 500\")\n",
    "    print(\"\\tThe third returned value is the un-remixed test set, length is 500\\n\")\n",
    "    \n",
    "    final_test_n = 500 # per assignment instructions\n",
    "    dev_test_n = 500 # per assignment instructions\n",
    "    \n",
    "    #reserve first 500 for the final test    \n",
    "    test_corpus = corpus[:final_test_n] \n",
    "    \n",
    "    #create a copy of the dev_set to preserve the original test set before shuffling\n",
    "    dev_set = corpus[final_test_n:] \n",
    "    random.shuffle(dev_set) #remix before re-slicing\n",
    "    \n",
    "    #re-cut re-shuffled development set into dev-test set (len 500) and training set (remainder)\n",
    "    dev_test_corpus = dev_set[:dev_test_n]\n",
    "    train_corpus = dev_set[dev_test_n:]\n",
    "    \n",
    "    #prints sample of sets\n",
    "    print(\"Training Corpus Sample: \",train_corpus[0:3], \", Length: \", len(train_corpus)) #should be longer\n",
    "    print(\"Dev-Test Corpus Sample: \",dev_test_corpus[0:3], \", Length: \", len(dev_test_corpus)) #should have length 500\n",
    "    print(\"Test Corpus Sample: \", test_corpus[0:3], \", Length: \", len(test_corpus))\n",
    "    \n",
    "    return train_corpus, dev_test_corpus, test_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is ```reslicer``` in action. Note that each call of this function will create a *new* mix of training and development-testing data, while the \"test_names\" will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reslicer returns 3 sliced, remixed set of corpuses:\n",
      "\tThe first returned value is the remixed training corpus, length is variable\n",
      "\tThe second returned value is the remixed dev-test corpus, length is 500\n",
      "\tThe third returned value is the un-remixed test set, length is 500\n",
      "\n",
      "Training Corpus Sample:  [('Arlee', 'female'), ('Karita', 'female'), ('Zola', 'female')] , Length:  6944\n",
      "Dev-Test Corpus Sample:  [('Shilpa', 'female'), ('Dusty', 'female'), ('Torey', 'female')] , Length:  500\n",
      "Test Corpus Sample:  [('Lucila', 'female'), ('Siana', 'female'), ('Emilio', 'male')] , Length:  500\n"
     ]
    }
   ],
   "source": [
    "train_names, dev_names, test_names = reslicer(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "To develop our model, first, we will combine the ```reslicer``` function with a feature extractor function, ```feature_ext```, to generate labeled data with features.\n",
    "\n",
    "Then we will develop a series of feature functions (used as the ```feature_func``` arg in the ```feature_ext``` function) to test the predictive value of various features using NLTK's built-in naive Bayes classifiers.\n",
    "\n",
    "A little info on naive Bayes classifiers from [Natural Language Processing with Python, Chapter 6](https://www.nltk.org/book/ch06.html):  \n",
    "\n",
    "\"In naive Bayes classifiers, every feature gets a say in determining which label should be assigned to a given input value. To choose a label for an input value, the naive Bayes classifier begins by calculating the prior probability of each label, which is determined by checking frequency of each label in the training set. The contribution from each feature is then combined with this prior probability, to arrive at a likelihood estimate for each label.\"\n",
    "\n",
    "\"Individual features make their contribution to the overall decision by \"voting against\" labels that don't occur with that feature very often. In particular, the likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to process the names through feature extractor\n",
    "def feature_ext(feature_func, corpus):\n",
    "    \n",
    "    #first, remix and reslice the data to ensure we are using a new mix of dev-test and training data each time\n",
    "    train_names, dev_names, test_names = reslicer(corpus)\n",
    "    \n",
    "    #then, extract features from the names slices\n",
    "    train_set = [(feature_func(n), gender) for (n, gender) in train_names]\n",
    "    devtest_set = [(feature_func(n), gender) for (n, gender) in dev_names]\n",
    "\n",
    "    \n",
    "    return train_set, devtest_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ```find_errors``` function and a ```test_model``` function will combine all of the above to provide feedback on the feature extraction method selected to develop a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(feature_func, classifier):\n",
    "    errors = {'name' : [], 'label' : [], 'guess' : [], 'features': [] }\n",
    "    for (name, label) in dev_names:\n",
    "            guess = classifier.classify(feature_func(name))\n",
    "            features = feature_func(name)\n",
    "            \n",
    "            if guess != label:\n",
    "                errors['name'].append(name)\n",
    "                errors['label'].append(label)\n",
    "                errors['guess'].append(guess)\n",
    "                errors['features'].append(features)\n",
    "        \n",
    "    errors = pd.DataFrame(errors)\n",
    "        \n",
    "    # Prints sample of errors\n",
    "    print(\"\\nErrors\")\n",
    "    print(errors.sample(20))\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(feature_func, corpus):\n",
    "    \n",
    "    # Run the feature_ext function to create the necessary labeled feature sets\n",
    "    train_set, devtest_set = feature_ext(feature_func, corpus)\n",
    "    \n",
    "    # Train on the training set using the naiveBayes classifier built in to nltk\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    # Test the accuracy of the classifier on the dev data--this is so we can evaluate errors and make tweaks\n",
    "    a = round(nltk.classify.accuracy(classifier, devtest_set), 4)*100\n",
    "    \n",
    "    # Format results as a 2 digit decimal\n",
    "    accuracy = f'{a:.2f}'\n",
    "    \n",
    "    # Print message with results\n",
    "    print(\"\\n\")\n",
    "    print(\"Model is %s percent accurate\" % accuracy)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Examine classifier to determine which last letter is most effective for predicting gender\n",
    "    print(classifier.show_most_informative_features(10))\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the last letter of the name\n",
    "\n",
    "Now that we have our functions set up, we can use them to test different feature extraction functions, starting with the last letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_letter(name): #first feature extraction function to test\n",
    "    \n",
    "    return {\"last_letter\": name[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reslicer returns 3 sliced, remixed set of corpuses:\n",
      "\tThe first returned value is the remixed training corpus, length is variable\n",
      "\tThe second returned value is the remixed dev-test corpus, length is 500\n",
      "\tThe third returned value is the un-remixed test set, length is 500\n",
      "\n",
      "Training Corpus Sample:  [('Lois', 'female'), ('Thornie', 'male'), ('Katharyn', 'female')] , Length:  6944\n",
      "Dev-Test Corpus Sample:  [('Salvidor', 'male'), ('Bartel', 'male'), ('Geri', 'male')] , Length:  500\n",
      "Test Corpus Sample:  [('Lucila', 'female'), ('Siana', 'female'), ('Emilio', 'male')] , Length:  500\n",
      "\n",
      "\n",
      "Model is 75.80 percent accurate\n",
      "\n",
      "\n",
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     35.9 : 1.0\n",
      "             last_letter = 'k'              male : female =     30.7 : 1.0\n",
      "             last_letter = 'f'              male : female =     15.2 : 1.0\n",
      "             last_letter = 'p'              male : female =     10.5 : 1.0\n",
      "             last_letter = 'd'              male : female =     10.0 : 1.0\n",
      "             last_letter = 'v'              male : female =      9.8 : 1.0\n",
      "             last_letter = 'm'              male : female =      8.9 : 1.0\n",
      "             last_letter = 'o'              male : female =      8.1 : 1.0\n",
      "             last_letter = 'r'              male : female =      6.8 : 1.0\n",
      "             last_letter = 'g'              male : female =      5.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "last_letter_model = test_model(last_letter, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are interesting results! Let's examine and visualize the error data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Errors\n",
      "         name   label   guess              features\n",
      "72        Kat  female    male  {'last_letter': 't'}\n",
      "78        Mel  female    male  {'last_letter': 'l'}\n",
      "34     Wolfie    male  female  {'last_letter': 'e'}\n",
      "116   Rhianon  female    male  {'last_letter': 'n'}\n",
      "61   Gretchen  female    male  {'last_letter': 'n'}\n",
      "115      Noah    male  female  {'last_letter': 'h'}\n",
      "49      Torey    male  female  {'last_letter': 'y'}\n",
      "99     Eugene    male  female  {'last_letter': 'e'}\n",
      "37      Jamey    male  female  {'last_letter': 'y'}\n",
      "76       Inez  female    male  {'last_letter': 'z'}\n",
      "104     Leigh    male  female  {'last_letter': 'h'}\n",
      "111     Welby    male  female  {'last_letter': 'y'}\n",
      "2    Marigold  female    male  {'last_letter': 'd'}\n",
      "65     Dallas  female    male  {'last_letter': 's'}\n",
      "47     Flower  female    male  {'last_letter': 'r'}\n",
      "38     Manish    male  female  {'last_letter': 'h'}\n",
      "32      Shawn  female    male  {'last_letter': 'n'}\n",
      "22      Wayne    male  female  {'last_letter': 'e'}\n",
      "83     Worthy    male  female  {'last_letter': 'y'}\n",
      "66      Karon  female    male  {'last_letter': 'n'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x115ae4b38>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = find_errors(last_letter, last_letter_model)\n",
    "#create dataframe for bar plot\n",
    "hister = pd.DataFrame(errors[['features','guess']])\n",
    "hister = pd.DataFrame(hister['features'].tolist(), index=hister.index)\n",
    "hister2 = pd.DataFrame(hister['last_letter'].value_counts())\n",
    "hister2 = hister2.sort_values('last_letter',ascending=False)\n",
    "hister2.reset_index(level=0, inplace=True)\n",
    "        \n",
    "#plot bar plot of errors caused by last letter\n",
    "hister2.plot('index','last_letter', kind='bar',figsize=(20,10), fontsize=14,title=\"Errors: Most Common Last Letter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ending in 'a' is the only letter in the top ten that predicts female names instead of male names. Depending on the run, either 'n' or 'e' is the last letter that prodices the most errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the last 3 letters of the name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try the last 3 letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'last_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-15c0c36ced2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#note, we are automatically re-slicing the training/dev-test slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlast3letters_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'last_3' is not defined"
     ]
    }
   ],
   "source": [
    "def last3letters(name):\n",
    "    return {\"last3letters\": name[-3:]}  # feature set\n",
    "\n",
    "#note, we are automatically re-slicing the training/dev-test slices\n",
    "last3letters_model = test_model(last_3, corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's examine errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = find_errors(last3letters, last3letters_model)\n",
    "#create dataframe for bar plot\n",
    "hister = pd.DataFrame(errors[['features','guess']])\n",
    "hister = pd.DataFrame(hister['features'].tolist(), index=hister.index)\n",
    "\n",
    "hister2 = pd.DataFrame(hister['last3letters'].value_counts())\n",
    "hister2 = hister2.sort_values('last3letters',ascending=False)\n",
    "        \n",
    "hister2.reset_index(level=0, inplace=True)\n",
    "        \n",
    "#plot bar plot of errors caused by last letter\n",
    "hister2.plot('index','last3letters', kind='bar',figsize=(20,10), fontsize=14,title=\"Errors: Last 3 Letters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, there is no stand-out here--the highest value error is names that end in \"nie\", but this only occurred 4 times. While it would be tempting to specifically dig into those 4 names, it wouldn't add much value to the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the last 3 letters and the last letter\n",
    "\n",
    "Finally, we try a combination of the last three letters as well as the last letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_features(name):\n",
    "    return {\"last_letter\": name[-1], \"last3letters\": name[-3:]}  # feature set\n",
    "\n",
    "#note, we are automatically re-slicing the training/dev-test slices\n",
    "two_features_model = test_model(two_features, corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = find_errors(two_features, two_features_model)\n",
    "        \n",
    "#create dataframe for bar plot\n",
    "hister = pd.DataFrame(errors[['features','guess']])\n",
    "hister = pd.DataFrame(hister['features'].tolist(), index=hister.index)\n",
    "\n",
    "hister2 = pd.DataFrame(hister['last3letters'].value_counts())\n",
    "hister3 = pd.DataFrame(hister['last_letter'].value_counts())\n",
    "        \n",
    "print(hister2.head())\n",
    "print(hister3.head())\n",
    "        \n",
    "hister2.reset_index(level=0, inplace=True)\n",
    "hister3.reset_index(level=0, inplace=True)\n",
    "hister2['feature'] = hister2['last3letters']\n",
    "hister3['feature'] = hister3['last_letter']\n",
    "        \n",
    "frames = [hister2,hister3]\n",
    "result = pd.concat(frames,sort=True)\n",
    "result = result.sort_values('feature',ascending=False)\n",
    "        \n",
    "#plot bar plot of errors caused by last letter\n",
    "result.plot('index','feature', kind='bar',figsize=(20,10), fontsize=14,title=\"Errors: Last 3 Letters and Last Letter\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the last three letters increased our accuracy by about 5 percentage points!\n",
    "\n",
    "It appears that names ending with the letter \"e\" are the most problematic, followed by a combination of \"n\", \"y\" and \"l\", just as when the model was run analyzing the last letter of the name exclusively. Similarly the 3-letter endings provide a similar error distribution.\n",
    "\n",
    "Interestingly, each time we re-run the model, the data is re-sliced, yielding slightly different results in both the model accuracy and the error information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three features\n",
    "   \n",
    "If two features are better than one, will three be even better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_features(name):\n",
    "    return {\"last_letter\": name[-1], \"last3letters\": name[-3:], \"first_letter\": name[0]}  # feature set\n",
    "\n",
    "#note, we are automatically re-slicing the training/dev-test slices\n",
    "three_features_model = test_model(three_features, corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this didn't make a difference at all! Looks like the two_features_model is the winner. We can now test our best model on the test data--which has so far not been used to train any of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on unused data (```test_names```)\n",
    "\n",
    "The final step is to use the classify the ```test_set``` using ```two_features_model``` to see how we did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test(classifier, feature_func):\n",
    "    \n",
    "    #generate test_set\n",
    "    test_set = [(feature_func(n), gender) for (n, gender) in test_names] #note that reslicer generated this var\n",
    "        \n",
    "    \n",
    "    #test the accuracy of the model on the test set\n",
    "    a = round(nltk.classify.accuracy(classifier, test_set), 4)*100\n",
    "    \n",
    "    #format and print output\n",
    "    accuracy = f'{a:.2f}'\n",
    "    print(\"\\n\")\n",
    "    print(\"Model is %s percent accurate when used on the the test set\" % accuracy)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "\n",
    "final_test(two_features_model, two_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, the final test does not produce identical results when run on the test set (or even re-run on the development set). In fact, it look almost 4 points worse!\n",
    "\n",
    "This should not be surprising because the model is making a prediction based on patterns that are not necessarily hard and fast rules. \n",
    "\n",
    "An interesting project for further study would be to add weights to the names based on the number of people who have each name so that more common names more heavily tilt the model. While this might not produce a more accurate result looking at a list of names, it should be more accurate when dealing with new, real-world data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
